---
title: 3. Install Steps
description: Installation guidance
sidebar_position: 3
---

## 3.1: Overview

This module focuses on the initial deployment of the Cloud Pak for AIOps using the Linux VM based installer. It will include the prerequisite steps of preparing the virtual machines prior to installation.

The hosts provided in this lab include the following (hostname, FQDN, purpose):
```
aiopsc1 aiopsc1.ibmdte.local - control plane node 1
aiopsc2 aiopsc2.ibmdte.local - control plane node 2
aiopsc3 aiopsc3.ibmdte.local - control plane node 3
aiopsw1 aiopsw1.ibmdte.local - worker node 1
aiopsw2 aiopsw2.ibmdte.local - worker node 2
aiopsw3 aiopsw3.ibmdte.local - worker node 3
aiopsw4 aiopsw4.ibmdte.local - worker node 4
aiopsw5 aiopsw5.ibmdte.local - worker node 5
aiopsw6 aiopsw6.ibmdte.local - worker node 6
aiopslb aiopslb.ibmdte.local - the load balancer host
```

:::note

An additional VM is included in your environment (`netcoolvm`) and is preinstalled with Netcool/OMNIbus and Netcool/Impact. It is included so that you can continue with the other labs included in this series. It is not used in this installation lab however.

:::

## 3.2: Prepare your hosts

The first step is to update the operating system on the set of VMs so that they are at the latest patch levels.

SSH to each box in your environment and update the systems per the following:
```
ssh jammer@aiopsc1
sudo su -
```
Run the following to update and reboot them:
```
yum update -y
shutdown -r now
```
Next, install the following package on the load balancer host (`aiopslb`):
```
yum install -y haproxy
```
Finally, install the following package on the control plane and worker nodes:
```
yum install -y lvm2
```
The control plane nodes are provisioned with two additional disks and the worker nodes with one additional disk. These must be prepared for use before attempting any installation. Follow the instructions on the link below to set these volumes up.

Documentation reference: [Configuring local volumes](https://www.ibm.com/docs/en/cloud-paks/cloud-pak-aiops/latest?topic=requirements-configuring-local-volumes)

:::caution

Do not proceed past this point until all the additional volumes have been configured. After completing this step, you should have two additional volumes on each of the control plane nodes and one additional volume on each of the worker nodes.

:::

## 3.3: Modify the hosts so that root can SSH to  them

The installation process connects to the hosts as the root user. The first step to enabling this is to update each host and allow SSH logins as the root user.

Connect to each of your hosts and `su` to the root user:
```
ssh jammer@aiopsc1
sudo su -
```
Run the following command to enable root SSH logins:
```
sed -i 's/^#? *PermitRootLogin.*/PermitRootLogin yes/' /etc/ssh/sshd_config
```

## 3.4: Set up an SSH key on the primary control plane node

The installation of AIOps is done through the primary control plane node. As such, you need to create an SSH key on the primary control node, and then add it to the authorized keys file on each of the other control plane and worker nodes.

Connect to your primary control plane node and `su` to the root user:
```
ssh jammer@aiopsc1
sudo su -
```
Run the following command on your primary control plane node to create an SSH key:
```
cd ~/.ssh
ssh-keygen -o
```

You should see a new file created called `id_rsa.pub`. Copy the contents of this file to your clipboard as you will need to paste it shortly.

## 3.5: Copy the SSH key to the other nodes and test

Next, SSH as the root user to each of the other control plane nodes as well as each of the worker nodes and append this copied key to the end of the `/root/.ssh/authorized_keys` file on each one.

After you have done so, test that you can SSH to each control plane and worker node from the primary control plane node without having to enter a password.

:::note

It is important to connect at least once to each control plane and worker node from the primary control plane node, and answer "yes" when prompted. If you miss out this step, the the AIOps deployment will likely fail later, as the installer will not be able to seamlessly connect to each node from the primary, as it will be awaiting an answer to this question behind the scenes.

:::

The following is an example first connection where the user is prompted to add the target host to the list of known hosts:
```
[root@aiopsc1 ~]# ssh root@aiopsc2
The authenticity of host 'aiopsc2 aiopsc2.ibmdte.local (::1)' can't be established.
ED25519 key fingerprint is SHA256:bOcJ27pUHfc/BVomiCixBsJK2ys0Z+LLBm3CdRMUy5o.
This key is not known by any other names
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added 'aiopsc2 aiopsc2.ibmdte.local' (ED25519) to the list of known hosts.
[root@aiopsc2 ~]# 
```
Subsequent connections will not require an answer (which is what you want):
```
[root@aiopsc1 ~]# ssh root@aiopsc2
Last login: Tue Nov  5 09:36:40 2024 from ::1
[root@aiopsc2 ~]# 
```
You will need to repeat this test connection from `aiopsc1` to each of the other control plane nodes and worker nodes until you can connect seamlessly to all without needing to enter a password:
```
ssh root@aiopsc2
ssh root@aiopsc3
ssh root@aiopsw1
ssh root@aiopsw2
ssh root@aiopsw3
ssh root@aiopsw4
ssh root@aiopsw5
ssh root@aiopsw6
```
## 3.6: Set up an SSH key on your deployment machine

This step involves creating an SSH key on the machine from which you will run the deployment from. In this lab, this will be the bastion host. Open a command prompt on your bastion host and change to the root user:

```
sudo su -
```

You should follow the same steps outlined above to create a public SSH key on your bastion host, then append the local `id_rsa.pub` key contents to the `/root/.ssh/authorized_keys` file on the *primary control plane node* (`aiopsc1`).

You should then be able to SSH to the primary control plane node from your bastion host without the need to enter a password. This is important since the installation steps you'll run later to deploy AIOps use the following format:

```
ssh ${TARGET_USER}@${CONTROL_PLANE_NODE} ssh ${TARGET_USER}@${CP_NODE} \
        curl -LO "${AIOPSCTL_INSTALL_URL}"
```

Here, the command first connects from the bastion host to the primary control plane node, then it runs a second SSH command to go the target node, then finally runs the designated command on that ultimate target - in this case a `curl` command.

Hence you need to be able to seamlessly SSH from your deployment machine to the primary control plane node (`aiopsc1`), then from the primary control plane node to all the other AIOps cluster nodes.

As before, test out your SSH connection to the primary control plane node and answer "yes" when prompted to ensure seamless connectivity thereafter.

## 3.7: Set up your load balancer

For resiliency, AIOps is designed to run using a load-balancer that proxies connections to its control plane nodes. If you are setting up AIOps in an evaluation environment where there is no load-balancer available, you can simply provision an extra VM and run `haproxy` on it.

In this lab, we have a load-balancer host provisioned, with haproxy already installed. Next, we need to set up the configuration file, then start up the service.

The following is a sample `haproxy.cfg` file you can use. The only thing you need to modify are the three IP addresses mentioned in the three places. The three IP addresses are simply those of the three control plane nodes. Hence use the following file as-is, except for the values of the three IP addresses. You do not need to change the hostnames or any other details in the file.

_Sample haproxy.cfg file:_
```
global
	log         127.0.0.1 local2
	chroot      /var/lib/haproxy
	pidfile     /var/run/haproxy.pid
	maxconn     4000
	user        haproxy
	group       haproxy
	daemon
	stats socket /var/lib/haproxy/stats

defaults
	mode                    http
	log                     global
	option                  httplog
	option                  dontlognull
	option http-server-close
	option forwardfor       except 127.0.0.0/8
	option                  redispatch
	retries                 3
	timeout http-request    10s
	timeout queue           1m
	timeout connect         10s
	timeout client          1m
	timeout server          1m
	timeout http-keep-alive 10s
	timeout check           10s
	maxconn                 3000

frontend aiops-frontend-plaintext
	bind *:80
	mode tcp
	option tcplog
	default_backend aiops-backend-plaintext

frontend aiops-frontend
	bind *:443
	mode tcp
	option tcplog
	default_backend aiops-backend

frontend k3s-frontend
	bind *:6443
	mode tcp
	option tcplog
	default_backend k3s-backend

backend aiops-backend
	mode tcp
	option tcp-check
	balance roundrobin
	default-server inter 10s downinter 5s
	server server0 aiopsc1:443 check
	server server1 aiopsc2:443 check
	server server2 aiopsc3:443 check

backend k3s-backend
	mode tcp
	option tcp-check
	balance roundrobin
	default-server inter 10s downinter 5s
	server server0 aiopsc1:6443 check
	server server1 aiopsc2:6443 check
	server server2 aiopsc3:6443 check

backend aiops-backend-plaintext
	mode tcp
	option tcp-check
	balance roundrobin
	default-server inter 10s downinter 5s
	server server0 aiopsc1:80 check
	server server1 aiopsc2:80 check
	server server2 aiopsc3:80 check
```
Copy your `haproxy.cfg` file to the `/tmp` directory on your load balancer host. Then, after backing up the original configuration file, copy your replacement file into place. Finally, enable and start the service:

```
ssh jammer@aiopslb
sudo su -
cd /etc/haproxy/
mv haproxy.cfg haproxy.cfg.orig
cp /tmp/haproxy.cfg .
systemctl enable haproxy
systemctl start haproxy
systemctl status haproxy
```
:::note

Don't worry if you see some errors in the status output at this point, since the AIOps end points don't exist yet.

:::

## 3.8: Follow the AIOps installation steps

Your VM environment is now ready to start the AIOps deployment. The deployment itself should take around an hour to complete.

Documentation reference: [Online installation of IBM Cloud Pak for AIOps on Linux](https://www.ibm.com/docs/en/cloud-paks/cloud-pak-aiops/latest?topic=linux-online-installation)

## 3.9: Log in to the Cloud Pak for AIOps console

The final step in the installation process reveals the Cluster Access Details.

Edit the `/etc/hosts` file on your bastion host and add the details of your newly installed AIOps instance. Note that this will be based on the FQDN of your load balancer host and its IP address:
```
<load_balancer_ip> aiops-cpd.<your fully-qualified server domain name>
<load_balancer_ip> cp-console-aiops.<your fully-qualified server domain name>
```
For this environment therefore, add the following to your local `/etc/hosts` file on your bastion host:
```
192.168.252.9 aiops-cpd.aiopslb.ibmdte.local
192.168.252.9 cp-console-aiops.aiopslb.ibmdte.local
```

:::note

Both of these need to be present in your bastion host `/etc/hosts` file for correct operation.

:::

Documentation reference: [DNS requirements](https://www.ibm.com/docs/en/cloud-paks/cloud-pak-aiops/4.11.0?topic=linux-planning#dns)

Finally, open a Firefox web browser session on your bastion host and go to the AIOps login page. 
```
https://aiops-cpd.aiopslb.ibmdte.local
```
Log in to AIOps using the credentials provided at the end of installation.

If you have mislaid the login credentials, simply SSH to one of the control plane nodes and run the following command:
```
ssh root@aiopsc1
aiopsctl server info --show-secrets
```

Congratulations, this concludes the installation of AIOps on Linux VMs!

